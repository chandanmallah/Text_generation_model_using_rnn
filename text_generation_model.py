# -*- coding: utf-8 -*-
"""Text generation model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FGL6I3kwOK9RIblC1U5HeH8VHyBtH1f5
"""

import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

# Downloading NLTK Data
nltk.download('punkt')
nltk.download('stopwords')

# Load the Gutenberg-Poetr dataset
file_path = '/Gutenberg-Poetry.csv'

df = pd.read_csv(file_path)

df.head()

# Function to preprocess and clean the text data
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)


    tokens = [token.lower() for token in tokens]


    tokens = [token for token in tokens if token not in string.punctuation]


    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]


    cleaned_text = " ".join(tokens)
    return cleaned_text

# Droping t he  rows with NaN in 's' column

df = df.dropna(subset=['s'])

df['processed_text'] = df['s'].apply(preprocess_text)

output_file_path = '/content/Gutenberg-Poetry.csv'
df.to_csv(output_file_path, index=False)

df.head()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split

# Loading the preprocessed dataset
file_path = '/content/Gutenberg-Poetry.csv'
df = pd.read_csv(file_path)

# Dropping rows with NaN in the 'processed_text' column
df = df.dropna(subset=['processed_text'])

# converting test int0 list
text_data = df['processed_text'].tolist()

tokenizer = Tokenizer()
tokenizer.fit_on_texts(text_data)
sequences = tokenizer.texts_to_sequences(text_data)
max_sequence_length = 50
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')

# training
input_data = np.array(padded_sequences)
target_data = np.roll(input_data, -1, axis=1)
target_data[:, -1] = 0
train_texts, val_texts, train_targets, val_targets = train_test_split(input_data, target_data, test_size=0.2, random_state=42)

# building RNN model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))
model.add(LSTM(128))
model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')

# Training the model
batch_size = 128
epochs = 20
Rnn_model = model.fit(train_texts, np.argmax(train_targets, axis=-1), validation_data=(val_texts, np.argmax(val_targets, axis=-1)), batch_size=batch_size, epochs=epochs)

#  Generating text using the trained model
start_sequence = "It is winter overe here"
input_sequence = tokenizer.texts_to_sequences([start_sequence])
input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='pre')

generated_sequence = []
for _ in range(10):
    predicted_word_probs = model.predict(input_sequence)[0]
    predicted_word_idx = np.argmax(predicted_word_probs)
    predicted_word = tokenizer.index_word[predicted_word_idx]
    generated_sequence.append(predicted_word)
    input_sequence = np.roll(input_sequence, -1, axis=1)
    input_sequence[0, -1] = predicted_word_idx

generated_text = start_sequence + " " + " ".join(generated_sequence)
print(generated_text)

model.save('text_generator.h5')



